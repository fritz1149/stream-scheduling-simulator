{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%load_ext blackcellmagic\r\n",
    "import sys\r\n",
    "import uuid\r\n",
    "sys.path.insert(0, \"..\")\r\n",
    "def gen_uuid():\r\n",
    "    return str(uuid.uuid4())[:8]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import algo\r\n",
    "import coloredlogs\r\n",
    "import graph\r\n",
    "import importlib\r\n",
    "import logging\r\n",
    "import math\r\n",
    "import networkx as nx\r\n",
    "import schedule as sch\r\n",
    "import random\r\n",
    "import topo\r\n",
    "import yaml\r\n",
    "importlib.reload(algo)\r\n",
    "importlib.reload(graph)\r\n",
    "importlib.reload(sch)\r\n",
    "importlib.reload(topo)\r\n",
    "coloredlogs.set_level(logging.INFO)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sc = topo.Scenario.from_dict(yaml.load(open(\"../samples/1e3h.yaml\", \"r\").read(), Loader=yaml.Loader))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "gen_args_list = [\r\n",
    "    {\r\n",
    "        \"graph_length\": random.randint(3, 11),\r\n",
    "        # \"mi_cb\": lambda: int(math.pow(10, (random.random() * 1) + 0)),\r\n",
    "        \"mi_cb\": lambda: 1,\r\n",
    "        \"memory_cb\": lambda: int(2e8),\r\n",
    "        \"unit_size_cb\": lambda: int(math.pow(10, (random.random() * 1) + 4)),\r\n",
    "        \"unit_rate_cb\": lambda: int(math.pow(10, (random.random() * 1) + 1)),\r\n",
    "        \"source_hosts\": [\"rasp1\", \"rasp2\", \"rasp3\"],\r\n",
    "        \"sink_hosts\": [\"cloud1\"],\r\n",
    "    }\r\n",
    "    for _ in range(10)\r\n",
    "]\r\n",
    "graph_list = [\r\n",
    "    graph.GraphGenerator(\"g\" + str(idx), **gen_args).gen_random_chain_graph()\r\n",
    "    for idx, gen_args in enumerate(gen_args_list)\r\n",
    "]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open(\"../cases/a.yaml\", \"w\") as f:\r\n",
    "    graph.ExecutionGraph.save_all(graph_list, f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bare implementation of min-cut scheduling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sc.topo.clear_occupied()\r\n",
    "flow_scheduler = sch.FlowScheduler(sc)\r\n",
    "flow_scheduler.logger.setLevel(logging.INFO)\r\n",
    "flow_calculator = sch.LatencyCalculator(sc.topo)\r\n",
    "flow_calculator.logger.setLevel(logging.INFO)\r\n",
    "flow_result_list = flow_scheduler.schedule_multiple(graph_list)\r\n",
    "for g, result in zip(graph_list, flow_result_list):\r\n",
    "    if result is None:\r\n",
    "        print('none')\r\n",
    "        continue\r\n",
    "    flow_calculator.add_scheduled_graph(g, result)\r\n",
    "flow_latency, flow_bp = flow_calculator.compute_latency()\r\n",
    "print(flow_latency)\r\n",
    "print(flow_bp)\r\n",
    "print(sum(flow_latency.values()))\r\n",
    "print(sum(flow_latency.values()) / len(flow_latency))\r\n",
    "print(sum(flow_bp.values()) / len(flow_bp))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## All cloud scheduling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sc.topo.clear_occupied()\r\n",
    "all_cloud_scheduler = sch.RandomScheduler(sc)\r\n",
    "all_cloud_scheduler.logger.setLevel(logging.INFO)\r\n",
    "all_cloud_calculator = sch.LatencyCalculator(sc.topo)\r\n",
    "all_cloud_calculator.logger.setLevel(logging.INFO)\r\n",
    "all_cloud_result_list = []\r\n",
    "s_graph_list = []\r\n",
    "t_graph_list = []\r\n",
    "for g in graph_list:\r\n",
    "    s_cut = set([v.uuid for v in g.get_sources()])\r\n",
    "    t_cut = set([v.uuid for v in g.get_sinks()]).union(set([v.uuid for v in g.get_operators()]))\r\n",
    "    s_graph_list.append(g.sub_graph(s_cut, gen_uuid()))\r\n",
    "    t_graph_list.append(g.sub_graph(t_cut, gen_uuid()))\r\n",
    "s_result_list = all_cloud_scheduler.schedule_multiple(s_graph_list, sc.get_edge_domains()[0].topo)\r\n",
    "t_result_list = all_cloud_scheduler.schedule_multiple(t_graph_list, sc.get_cloud_domains()[0].topo)\r\n",
    "for g, s_result, t_result in zip(graph_list, s_result_list, t_result_list):\r\n",
    "    if s_result.status == sch.SchedulingResultStatus.FAILED:\r\n",
    "        print(\"s_graph {} failed: {}\".format(g.uuid, s_result.reason))\r\n",
    "        continue\r\n",
    "    if t_result.status == sch.SchedulingResultStatus.FAILED:\r\n",
    "        print(\"t_graph {} failed: {}\".format(g.uuid, t_result.reason))\r\n",
    "        continue\r\n",
    "    result = sch.SchedulingResult.merge(s_result, t_result)\r\n",
    "    all_cloud_calculator.add_scheduled_graph(g, result)\r\n",
    "all_cloud_latency, all_cloud_bp = all_cloud_calculator.compute_latency()\r\n",
    "print(all_cloud_latency)\r\n",
    "print(all_cloud_bp)\r\n",
    "print(sum(all_cloud_latency.values()))\r\n",
    "print(sum(all_cloud_latency.values()) / len(all_cloud_latency))\r\n",
    "print(sum(all_cloud_bp.values()) / len(all_cloud_bp))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Global random scheduling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sc.topo.clear_occupied()\r\n",
    "random_scheduler = sch.RandomScheduler(sc)\r\n",
    "random_scheduler.logger.setLevel(logging.INFO)\r\n",
    "random_calculator = sch.LatencyCalculator(sc.topo)\r\n",
    "random_calculator.logger.setLevel(logging.INFO)\r\n",
    "random_result_list = []\r\n",
    "result_list = random_scheduler.schedule_multiple(graph_list, sc.topo)\r\n",
    "for g, result in zip(graph_list, result_list):\r\n",
    "    if result.status == sch.SchedulingResultStatus.FAILED:\r\n",
    "        print(\"graph {} failed: {}\".format(g.uuid, result.reason))\r\n",
    "        continue\r\n",
    "    random_calculator.add_scheduled_graph(g, result)\r\n",
    "random_latency, random_bp = random_calculator.compute_latency()\r\n",
    "print(random_latency)\r\n",
    "print(random_bp)\r\n",
    "print(sum(random_latency.values()))\r\n",
    "print(sum(random_latency.values()) / len(random_latency))\r\n",
    "print(sum(random_bp.values()) / len(random_bp))"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "add1a61ff37513b0d798fa9b3ddbf8ae573dfe73fde377561c8d981e86c7d8d4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}